
# ğŸ¤– Service ML d'Iris - DÃ©ploiement MLOps Complet

Ce projet dÃ©montre le dÃ©ploiement d'un service de prÃ©diction de Machine Learning (modÃ¨le Iris) Ã  l'aide d'un stack MLOps moderne et conteneurisÃ©. L'objectif est de garantir l'**observabilitÃ©** et la **flexibilitÃ©** en production.

## ğŸŒŸ CaractÃ©ristiques Principales

  * **API ML Productionnelle :** Service de prÃ©diction haute performance construit avec **FastAPI**.
  * **Conteneurisation :** Isolation et reproductibilitÃ© de l'environnement grÃ¢ce Ã  **Docker**.
  * **Orchestration :** DÃ©ploiement multi-services (API, Prometheus, Grafana) gÃ©rÃ© par **Docker Compose**.
  * **Configuration SÃ©curisÃ©e :** Utilisation de **Pydantic Settings** et du fichier **`.env`** pour externaliser les variables de configuration (version, secrets) de l'image Docker.
  * **ObservabilitÃ© / Monitoring :** IntÃ©gration complÃ¨te de **Prometheus** (collecte de mÃ©triques) et **Grafana** (visualisation) pour surveiller la santÃ© et les performances de l'API en temps rÃ©el.

-----

## ğŸ› ï¸ Stack Technique

| Composant | RÃ´le |
| :--- | :--- |
| **Framework API** | FastAPI / Uvicorn |
| **Conteneurisation** | Docker |
| **Orchestration** | Docker Compose |
| **ModÃ¨les** | Scikit-learn (fichiers `.pkl`) |
| **Monitoring** | Prometheus |
| **Visualisation** | Grafana |
| **Gestion Config** | Pydantic Settings / `.env` |

-----

## ğŸš€ DÃ©marrage Rapide

Ces instructions vous permettent de construire et de lancer l'intÃ©gralitÃ© du stack sur votre machine.

### 1\. PrÃ©requis

Assurez-vous d'avoir installÃ© sur votre systÃ¨me :

  * **Docker Desktop** (recommandÃ©, avec WSL activÃ© si sur Windows).
  * **Git**.

### 2\. Construction de l'Image Docker

Nous construisons l'image de l'API en incluant l'instrumentation Prometheus.

```bash
# Construire l'image de l'API ML (version 3.1)
docker build -t fastapi-ml-service:v3.1 .
```

### 3\. Lancement du Stack Complet

Cette commande lit le `docker-compose.yml`, dÃ©marre l'API, Prometheus et Grafana, et attache le fichier de configuration `.env`.

```bash
# Lancer l'API, Prometheus et Grafana en mode dÃ©tachÃ© (-d)
docker compose up -d
```

-----

## ğŸ”— Endpoints et Configuration

Une fois le stack lancÃ© (cela peut prendre quelques secondes pour que l'API dÃ©marre), les services sont accessibles via les adresses suivantes :

| Service | Adresse | DÃ©tails |
| :--- | :--- | :--- |
| **API ML** | `http://localhost:8000/docs` | Documentation interactive (Swagger UI) pour les endpoints de prÃ©diction. |
| **MÃ©triques API** | `http://localhost:8000/metrics` | Endpoint des mÃ©triques au format Prometheus. |
| **Prometheus** | `http://localhost:9090` | Interface de consultation et vÃ©rification des cibles. |
| **Grafana** | `http://localhost:3000` | Tableaux de bord de visualisation. |

### Configuration de la Source de DonnÃ©es Grafana

1.  AccÃ©dez Ã  **Grafana** (`http://localhost:3000`) et connectez-vous (**admin / admin**).
2.  Allez dans **Connections** \> **Data Sources** et ajoutez une source de donnÃ©es **Prometheus**.
3.  Dans le champ **URL**, utilisez l'adresse interne pour la communication entre conteneurs :
    $$\text{http://prometheus:9090}$$
    *(Si cette adresse Ã©choue, essayez l'adresse de l'hÃ´te Docker : `http://host.docker.internal:9090`)*.

-----

## ğŸ§¹ Nettoyage

Pour arrÃªter et supprimer tous les conteneurs et le rÃ©seau crÃ©Ã© par Docker Compose :

```bash
# ArrÃªter et supprimer les conteneurs
docker compose down
```